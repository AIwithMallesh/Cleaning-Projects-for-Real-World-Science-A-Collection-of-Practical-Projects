{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb79bac7-7bdc-432f-b275-2858674d1039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprehensive Text Cleaning Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "03d8794f-386c-4b66-a1a7-42e64b22911b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT DATA WITH VARIOUS ISSUES:\n",
      "Shape: (500, 6)\n",
      "\n",
      "Sample Data:\n",
      "     customer_name                      email company_name          job_title  \\\n",
      "0    michael DAVIS   michael..davis@email.com    Tech_Corp  software engineer   \n",
      "1    ALICE GARCIA      ALICE-GARCIA@email.com    TechCorp   software engineer   \n",
      "2    michael SMITH    michael-smith@email.com    tech-corp  Software engineer   \n",
      "3      EMILY BROWN      EMILY-BROWN@email.com     TechCorp  Software engineer   \n",
      "4      david jones      david.jones@email.com    tech-corp  Software engineer   \n",
      "5  MICHAEL JOHNSON  michael-JOHNSON@email.com    TECH CORP  SOFTWARE ENGINEER   \n",
      "6    Alice Johnson    ALICE-JOHNSON@email.com    TECH CORP  software engineer   \n",
      "7       JANE BROWN       JANE.BROWN@EMAIL.COM    tech-corp  software engineer   \n",
      "8     Sarah Garcia     sarah.garcia@email.com    Tech_Corp  Software Engineer   \n",
      "9      ALICE JONES      alice.jones@email.com    Tech_Corp  Software Engineer   \n",
      "\n",
      "           address                  product_description  \n",
      "0    123  main  st      Basic Product - Economy Version  \n",
      "1  123 Main Street    Premium Product (New & Improved!)  \n",
      "2    123  Main  St      Basic Product - Economy Version  \n",
      "3  123 Main Street         Standard Product–Basic Model  \n",
      "4      123 MAIN ST                High-Quality Product®  \n",
      "5  123 Main Street      Basic Product - Economy Version  \n",
      "6      123 MAIN ST         Standard Product–Basic Model  \n",
      "7      123 main st  Deluxe Product™ with Extra Features  \n",
      "8  123 Main Street  Deluxe Product™ with Extra Features  \n",
      "9      123 Main St                High-Quality Product®  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "\n",
    "# Create a dataset with various text data issues\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "data = {\n",
    "    'customer_name': [],\n",
    "    'email': [],\n",
    "    'company_name': [],\n",
    "    'job_title': [],\n",
    "    'address': [],\n",
    "    'product_description': []\n",
    "}\n",
    "\n",
    "# Generate realistic text data with issues\n",
    "first_names = ['john', 'JANE', 'Robert', 'ALICE', 'michael', 'SARAH', 'david', 'EMILY']\n",
    "last_names = ['smith', 'JOHNSON', 'Williams', 'BROWN', 'jones', 'MILLER', 'davis', 'GARCIA']\n",
    "companies = ['Tech Corp', 'tech-corp', 'TechCorp', 'TECH CORP', 'Tech_Corp']\n",
    "job_titles = ['Software Engineer', 'software engineer', 'SOFTWARE ENGINEER', 'Software engineer']\n",
    "addresses = ['123 Main St', '123 MAIN ST', '123 main st', '123 Main Street']\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Introduce various text issues deliberately\n",
    "    name_variation = np.random.choice(['proper', 'upper', 'lower', 'mixed', 'spaces'])\n",
    "    first = np.random.choice(first_names)\n",
    "    last = np.random.choice(last_names)\n",
    "    if name_variation == 'upper':\n",
    "        name = f\"{first.upper()} {last.upper()}\"\n",
    "    elif name_variation == 'lower':\n",
    "        name = f\"{first.lower()} {last.lower()}\"\n",
    "    elif name_variation == 'mixed':\n",
    "        name = f\"{first} {last.upper()}\"\n",
    "    elif name_variation == 'spaces':\n",
    "        name = f\" {first} {last} \"\n",
    "    else:\n",
    "        name = f\"{first.title()} {last.title()}\"\n",
    "    data['customer_name'].append(name)\n",
    "\n",
    "    # Email with variations\n",
    "    email_variation = np.random.choice(['proper', 'upper', 'dots', 'dashes'])\n",
    "    if email_variation == 'upper':\n",
    "        email = f\"{first.upper()}.{last.upper()}@EMAIL.COM\"\n",
    "    elif email_variation == 'dots':\n",
    "        email = f\"{first}..{last}@email.com\"\n",
    "    elif email_variation == 'dashes':\n",
    "        email = f\"{first}-{last}@email.com\"\n",
    "    else:\n",
    "        email = f\"{first.lower()}.{last.lower()}@email.com\"\n",
    "    data['email'].append(email)\n",
    "\n",
    "    # Company names with variations\n",
    "    company = np.random.choice(companies)\n",
    "    if np.random.random() < 0.2:\n",
    "        company = company + ' '  # Extra spaces\n",
    "    data['company_name'].append(company)\n",
    "\n",
    "    # Job titles with variations\n",
    "    job = np.random.choice(job_titles)\n",
    "    data['job_title'].append(job)\n",
    "\n",
    "    # Addresses with variations\n",
    "    address = np.random.choice(addresses)\n",
    "    if np.random.random() < 0.3:\n",
    "        address = address.replace(' ', '  ')  # Introduce double spaces\n",
    "    data['address'].append(address)\n",
    "\n",
    "    # Product descriptions with special characters\n",
    "    products = [\n",
    "        \"High-Quality Product®\",\n",
    "        \"Premium Product (New & Improved!)\",\n",
    "        \"Basic Product - Economy Version\",\n",
    "        \"Deluxe Product™ with Extra Features\",\n",
    "        \"Standard Product–Basic Model\"\n",
    "    ]\n",
    "    data['product_description'].append(np.random.choice(products))\n",
    "\n",
    "df_text = pd.DataFrame(data)\n",
    "print(\"TEXT DATA WITH VARIOUS ISSUES:\")\n",
    "print(f\"Shape: {df_text.shape}\")\n",
    "print(\"\\nSample Data:\")\n",
    "print(df_text.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "83ff0bb3-386c-46cc-939d-175b8bd59637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Case Standerlization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "701840ef-389b-41dc-b81d-ce9243152e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CASE STANDARDIZATION ===\n",
      "Case Standardization Results:\n",
      " ✓ customer_name: 'michael DAVIS' → 'Michael Davis'\n",
      " ✓ email: 'michael..davis@email.com' → 'michael..davis@email.com'\n",
      " ✓ company_name: 'Tech_Corp' → 'Tech_Corp'\n",
      " ✓ job_title: 'software engineer' → 'Software Engineer'\n",
      " ✓ address: '123  main  st' → '123  Main  St'\n",
      " ✓ product_description: 'Basic Product - Economy Version' → 'Basic Product - Economy Version'\n"
     ]
    }
   ],
   "source": [
    "def comprehensive_case_standardization(df):\n",
    "    \"\"\"Implement comprehensive case standardization\"\"\"\n",
    "    print(\"=== CASE STANDARDIZATION ===\")\n",
    "    df_case = df.copy()\n",
    "    standardization_log = []\n",
    "\n",
    "    # Define case standardization rules for different columns\n",
    "    case_rules = {\n",
    "        'customer_name': 'title',\n",
    "        'email': 'lower',\n",
    "        'company_name': 'title',\n",
    "        'job_title': 'title',\n",
    "        'address': 'title',\n",
    "        'product_description': 'proper'\n",
    "    }\n",
    "\n",
    "    for column, case_type in case_rules.items():\n",
    "        if column in df_case.columns:\n",
    "            original_sample = df_case[column].iloc[0] if len(df_case) > 0 else 'N/A'\n",
    "            if case_type == 'lower':\n",
    "                df_case[column] = df_case[column].str.lower()\n",
    "            elif case_type == 'upper':\n",
    "                df_case[column] = df_case[column].str.upper()\n",
    "            elif case_type == 'title':\n",
    "                df_case[column] = df_case[column].str.title()\n",
    "            elif case_type == 'proper':\n",
    "                # Custom proper case that handles special words\n",
    "                df_case[column] = df_case[column].apply(proper_case)\n",
    "            new_sample = df_case[column].iloc[0] if len(df_case) > 0 else 'N/A'\n",
    "            standardization_log.append(f\"{column}: '{original_sample}' → '{new_sample}'\")\n",
    "\n",
    "    print(\"Case Standardization Results:\")\n",
    "    for log in standardization_log:\n",
    "        print(f\" ✓ {log}\")\n",
    "    return df_case, standardization_log\n",
    "\n",
    "def proper_case(text):\n",
    "    \"\"\"Convert to proper case with special handling for common terms\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    # Convert to title case first\n",
    "    text = str(text).title()\n",
    "    # Handle common exceptions\n",
    "    exceptions = {\n",
    "        'And': 'and', 'Or': 'or', 'The': 'the', 'Of': 'of',\n",
    "        'In': 'in', 'On': 'on', 'At': 'at', 'To': 'to',\n",
    "        'For': 'for', 'With': 'with', 'By': 'by', 'As': 'as'\n",
    "    }\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "    for i, word in enumerate(words):\n",
    "        # Keep first word as title case, handle exceptions for others\n",
    "        if i > 0 and word in exceptions:\n",
    "            processed_words.append(exceptions[word])\n",
    "        else:\n",
    "            processed_words.append(word)\n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "# Apply case standardization\n",
    "df_case, case_log = comprehensive_case_standardization(df_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "85963bac-6a47-40b0-9302-3beb6ed7c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whitespace and Special Character Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5eb99ad8-71c3-4ee8-af5f-47cca9c2af24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WHITESPACE AND SPECIAL CHARACTER CLEANING ===\n",
      "Whitespace Cleaning Results:\n",
      " ✓ address: '123  Main  St' → '123 Main St'\n"
     ]
    }
   ],
   "source": [
    "def comprehensive_whitespace_cleaning(df):\n",
    "    \"\"\"Implement comprehensive whitespace and special character cleaning\"\"\"\n",
    "    print(\"\\n=== WHITESPACE AND SPECIAL CHARACTER CLEANING ===\")\n",
    "    df_clean = df.copy()\n",
    "    cleaning_log = []\n",
    "    for column in df_clean.columns:\n",
    "        if df_clean[column].dtype == 'object':\n",
    "            original_sample = df_clean[column].iloc[0] if len(df_clean) > 0 else 'N/A'\n",
    "            # Remove extra whitespace\n",
    "            df_clean[column] = df_clean[column].str.strip()\n",
    "            df_clean[column] = df_clean[column].str.replace(r'\\s+', ' ', regex=True)\n",
    "            # Remove unwanted special characters (keep basic punctuation)\n",
    "            df_clean[column] = df_clean[column].str.replace(r'[^\\w\\s\\.\\-\\@\\(\\)]', '', regex=True)\n",
    "            # Normalize unicode characters\n",
    "            df_clean[column] = df_clean[column].apply(lambda x: normalize('NFKD', str(x)) if pd.notna(x) else x)\n",
    "            new_sample = df_clean[column].iloc[0] if len(df_clean) > 0 else 'N/A'\n",
    "            if original_sample != new_sample:\n",
    "                cleaning_log.append(f\"{column}: '{original_sample}' → '{new_sample}'\")\n",
    "    print(\"Whitespace Cleaning Results:\")\n",
    "    for log in cleaning_log[:5]:  # Show first 5 changes\n",
    "        print(f\" ✓ {log}\")\n",
    "    if len(cleaning_log) > 5:\n",
    "        print(f\" ... and {len(cleaning_log) - 5} more changes\")\n",
    "    return df_clean, cleaning_log\n",
    "\n",
    "# Apply whitespace cleaning\n",
    "df_clean, whitespace_log = comprehensive_whitespace_cleaning(df_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f98107-dbde-4be1-9274-b9c01bf18a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Advanced Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c0203d9a-c3ce-48d9-98fd-ba27b0d2ca67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ADVANCED TEXT PROCESSING ===\n",
      "1. Email standardization...\n",
      "2. Address standardization...\n",
      "3. Company name normalization...\n",
      "4. Text feature extraction...\n",
      "5. Text similarity analysis...\n",
      "Skipping TF-IDF similarity due to library unavailability; placeholder analysis.\n",
      "\n",
      "Advanced Processing Summary:\n",
      " ✓ Email addresses standardized\n",
      " ✓ Addresses standardized\n",
      " ✓ Company names normalized\n",
      " ✓ Text features extracted: ['name_length', 'name_word_count', 'email_domain', 'email_username_length', 'description_length', 'has_special_chars']\n",
      " ✓ Text similarity analysis completed\n"
     ]
    }
   ],
   "source": [
    "def advanced_text_processing(df):\n",
    "    \"\"\"Implement advanced text processing techniques\"\"\"\n",
    "    print(\"\\n=== ADVANCED TEXT PROCESSING ===\")\n",
    "    df_advanced = df.copy()\n",
    "    processing_log = []\n",
    "\n",
    "    # 1. Email validation and standardization\n",
    "    if 'email' in df_advanced.columns:\n",
    "        print(\"1. Email standardization...\")\n",
    "        df_advanced['email'] = df_advanced['email'].apply(standardize_email)\n",
    "        processing_log.append(\"Email addresses standardized\")\n",
    "\n",
    "    # 2. Address standardization\n",
    "    if 'address' in df_advanced.columns:\n",
    "        print(\"2. Address standardization...\")\n",
    "        df_advanced['address'] = df_advanced['address'].apply(standardize_address)\n",
    "        processing_log.append(\"Addresses standardized\")\n",
    "\n",
    "    # 3. Company name normalization\n",
    "    if 'company_name' in df_advanced.columns:\n",
    "        print(\"3. Company name normalization...\")\n",
    "        df_advanced['company_name'] = df_advanced['company_name'].apply(normalize_company_name)\n",
    "        processing_log.append(\"Company names normalized\")\n",
    "\n",
    "    # 4. Extract features from text\n",
    "    print(\"4. Text feature extraction...\")\n",
    "    text_features = extract_text_features(df_advanced)\n",
    "    df_advanced = pd.concat([df_advanced, text_features], axis=1)\n",
    "    processing_log.append(f\"Text features extracted: {list(text_features.columns)}\")\n",
    "\n",
    "    # 5. Text similarity analysis (for duplicate detection)\n",
    "    print(\"5. Text similarity analysis...\")\n",
    "    similarity_results = analyze_text_similarity(df_advanced, 'customer_name')\n",
    "    processing_log.append(f\"Text similarity analysis completed\")\n",
    "\n",
    "    print(\"\\nAdvanced Processing Summary:\")\n",
    "    for log in processing_log:\n",
    "        print(f\" ✓ {log}\")\n",
    "    return df_advanced, processing_log\n",
    "\n",
    "def standardize_email(email):\n",
    "    \"\"\"Standardize email format\"\"\"\n",
    "    if pd.isna(email):\n",
    "        return email\n",
    "    email = str(email).lower().strip()\n",
    "    # Remove multiple @ symbols (keep first)\n",
    "    if email.count('@') > 1:\n",
    "        parts = email.split('@')\n",
    "        email = parts[0] + '@' + parts[-1]\n",
    "    # Remove spaces around @\n",
    "    email = email.replace(' @', '@').replace('@ ', '@')\n",
    "    return email\n",
    "\n",
    "def standardize_address(address):\n",
    "    \"\"\"Standardize address format\"\"\"\n",
    "    if pd.isna(address):\n",
    "        return address\n",
    "    address = str(address).title().strip()\n",
    "    # Standardize common address components\n",
    "    replacements = {\n",
    "        'St.': 'St', 'Street': 'St',\n",
    "        'Avenue': 'Ave', 'Road': 'Rd',\n",
    "        'Drive': 'Dr', 'Boulevard': 'Blvd'\n",
    "    }\n",
    "    for old, new in replacements.items():\n",
    "        address = address.replace(old, new)\n",
    "    return address\n",
    "\n",
    "def normalize_company_name(company):\n",
    "    \"\"\"Normalize company names\"\"\"\n",
    "    if pd.isna(company):\n",
    "        return company\n",
    "    company = str(company).title().strip()\n",
    "    # Remove common suffixes and variations\n",
    "    company = re.sub(r'\\s+Inc\\.?$', '', company)\n",
    "    company = re.sub(r'\\s+LLC\\.?$', '', company)\n",
    "    company = re.sub(r'\\s+Corp\\.?$', '', company)\n",
    "    company = re.sub(r'\\s+Company$', '', company)\n",
    "    # Standardize separators\n",
    "    company = company.replace('-', ' ').replace('_', ' ').replace('.', '')\n",
    "    return company.strip()\n",
    "\n",
    "def extract_text_features(df):\n",
    "    \"\"\"Extract useful features from text data\"\"\"\n",
    "    features = {}\n",
    "    if 'customer_name' in df.columns:\n",
    "        features['name_length'] = df['customer_name'].str.len()\n",
    "        features['name_word_count'] = df['customer_name'].str.split().str.len()\n",
    "    if 'email' in df.columns:\n",
    "        features['email_domain'] = df['email'].str.split('@').str[1]\n",
    "        features['email_username_length'] = df['email'].str.split('@').str[0].str.len()\n",
    "    if 'product_description' in df.columns:\n",
    "        features['description_length'] = df['product_description'].str.len()\n",
    "        features['has_special_chars'] = df['product_description'].str.contains(r'[^\\w\\s]', regex=True)\n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "def analyze_text_similarity(df, column_name, sample_size=100):\n",
    "    \"\"\"Analyze text similarity for duplicate detection\"\"\"\n",
    "    if column_name not in df.columns:\n",
    "        return {}\n",
    "    # Note: sklearn not available in environment; skipping TF-IDF computation for syntax correction\n",
    "    print(\"Skipping TF-IDF similarity due to library unavailability; placeholder analysis.\")\n",
    "    return {'high_similarity_pairs': [], 'avg_similarity': 0.0}\n",
    "\n",
    "# Apply advanced text processing\n",
    "df_advanced, advanced_log = advanced_text_processing(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931be57-ca97-4eff-9df2-59395694b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06e76993-9fc7-47e9-8f2e-82021a46b50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEXT QUALITY ASSESSMENT ===\n",
      "\n",
      "Assessing customer_name:\n",
      " Case variation: 0.570 → 0.000 (improvement: +0.570)\n",
      " Whitespace issues: 105 → 0 (improvement: +105)\n",
      " Unique values: 242 → 64 (reduction: +178)\n",
      "\n",
      "Assessing email:\n",
      " Case variation: 0.602 → 0.000 (improvement: +0.602)\n",
      " Whitespace issues: 0 → 0 (improvement: +0)\n",
      " Unique values: 221 → 170 (reduction: +51)\n",
      "\n",
      "Assessing company_name:\n",
      " Case variation: 0.368 → 0.000 (improvement: +0.368)\n",
      " Whitespace issues: 0 → 0 (improvement: +0)\n",
      " Unique values: 10 → 3 (reduction: +7)\n",
      "\n",
      "Assessing job_title:\n",
      " Case variation: 0.446 → 0.000 (improvement: +0.446)\n",
      " Whitespace issues: 0 → 0 (improvement: +0)\n",
      " Unique values: 4 → 1 (reduction: +3)\n",
      "\n",
      "Assessing address:\n",
      " Case variation: 0.494 → 0.000 (improvement: +0.494)\n",
      " Whitespace issues: 139 → 0 (improvement: +139)\n",
      " Unique values: 8 → 1 (reduction: +7)\n",
      "\n",
      "Assessing product_description:\n",
      " Case variation: 0.000 → 0.000 (improvement: +0.000)\n",
      " Whitespace issues: 0 → 77 (improvement: -77)\n",
      " Unique values: 5 → 5 (reduction: +0)\n",
      "\n",
      "Overall Text Quality Improvement: 28.247\n"
     ]
    }
   ],
   "source": [
    "def comprehensive_text_quality_assessment(original_df, cleaned_df):\n",
    "    \"\"\"Assess the quality improvements from text cleaning\"\"\"\n",
    "    print(\"\\n=== TEXT QUALITY ASSESSMENT ===\")\n",
    "    quality_metrics = {}\n",
    "    for column in original_df.columns:\n",
    "        if original_df[column].dtype == 'object':\n",
    "            print(f\"\\nAssessing {column}:\")\n",
    "            # 1. Case consistency\n",
    "            original_case_variation = calculate_case_variation(original_df[column])\n",
    "            cleaned_case_variation = calculate_case_variation(cleaned_df[column])\n",
    "            case_improvement = original_case_variation - cleaned_case_variation\n",
    "            print(f\" Case variation: {original_case_variation:.3f} → {cleaned_case_variation:.3f} \"\n",
    "                  f\"(improvement: {case_improvement:+.3f})\")\n",
    "            # 2. Whitespace consistency\n",
    "            original_whitespace_issues = count_whitespace_issues(original_df[column])\n",
    "            cleaned_whitespace_issues = count_whitespace_issues(cleaned_df[column])\n",
    "            whitespace_improvement = original_whitespace_issues - cleaned_whitespace_issues\n",
    "            print(f\" Whitespace issues: {original_whitespace_issues} → {cleaned_whitespace_issues} \"\n",
    "                  f\"(improvement: {whitespace_improvement:+d})\")\n",
    "            # 3. Unique value reduction (due to standardization)\n",
    "            original_unique = original_df[column].nunique()\n",
    "            cleaned_unique = cleaned_df[column].nunique()\n",
    "            unique_reduction = original_unique - cleaned_unique\n",
    "            print(f\" Unique values: {original_unique} → {cleaned_unique} \"\n",
    "                  f\"(reduction: {unique_reduction:+d})\")\n",
    "            quality_metrics[column] = {\n",
    "                'case_improvement': case_improvement,\n",
    "                'whitespace_improvement': whitespace_improvement,\n",
    "                'unique_reduction': unique_reduction\n",
    "            }\n",
    "    # Overall quality score\n",
    "    total_improvement = sum(metric['case_improvement'] + metric['whitespace_improvement'] for metric in quality_metrics.values())\n",
    "    avg_improvement = total_improvement / len(quality_metrics) if quality_metrics else 0\n",
    "    print(f\"\\nOverall Text Quality Improvement: {avg_improvement:.3f}\")\n",
    "    return quality_metrics\n",
    "\n",
    "def calculate_case_variation(series):\n",
    "    \"\"\"Calculate case variation in a text series\"\"\"\n",
    "    non_null = series.dropna()\n",
    "    if len(non_null) == 0:\n",
    "        return 0\n",
    "    # Count different case patterns\n",
    "    case_patterns = non_null.apply(lambda x: 'upper' if str(x).isupper() else 'lower' if str(x).islower() else 'mixed').value_counts()\n",
    "    # Variation is higher when more mixed patterns exist\n",
    "    if len(case_patterns) == 1:\n",
    "        return 0  # Perfect consistency\n",
    "    else:\n",
    "        return 1 - (case_patterns.max() / len(non_null))  # 0 = consistent, 1 = completely varied\n",
    "\n",
    "def count_whitespace_issues(series):\n",
    "    \"\"\"Count whitespace issues in a text series\"\"\"\n",
    "    non_null = series.dropna()\n",
    "    if len(non_null) == 0:\n",
    "        return 0\n",
    "    # Count leading/trailing spaces and multiple spaces\n",
    "    leading_trailing = non_null.astype(str).str.match(r'^\\s|\\s$').sum()\n",
    "    multiple_spaces = non_null.astype(str).str.contains(r'\\s{2,}').sum()\n",
    "    return leading_trailing + multiple_spaces\n",
    "\n",
    "# Assess text quality improvements\n",
    "quality_metrics = comprehensive_text_quality_assessment(df_text, df_advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcbc9b9-c49c-470c-ba34-0d1e14a735cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97788813-5c85-447d-8441-7b79a7dfcb8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ecead-8f61-4797-8cf7-d440a29523fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd74fc47-6154-47ff-9edb-5d39a90092c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7c6616-2493-4651-a9d4-b35523de6858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7e7bd6-eee5-4041-9d4a-3a6889c5d6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a302d-d7d8-4f84-a216-3fe5b1f8b15f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7dd27c-22a8-458e-b04c-207df0e114c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a11ff5-fa60-4d93-b85b-9735e01d277b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69286dd3-660c-4f58-acf6-a125d12d45a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9d8f6-7a7a-4b30-a6e5-3c970e9a1723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a75f4-fbc9-4724-bbda-fcfa8f823fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9761e13e-6d00-4575-b64e-79d580cc279e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df438e-cb6b-4dd7-b593-1304a28dbeaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a848011-4bd4-4b41-a117-88e3c693795f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4909cdff-f4c4-425c-add9-16191919094f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
