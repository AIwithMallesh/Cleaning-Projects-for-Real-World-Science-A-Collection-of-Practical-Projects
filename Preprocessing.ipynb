{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8044c333-38fd-4b3c-943a-06b4489ae1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7509c014-68e6-4432-9f9e-4f50830e3f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90bdcc5a-7957-4400-aaf4-318fa9bd8ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " [[    25  50000]\n",
      " [    45 120000]\n",
      " [    65  80000]]\n",
      "\n",
      "Scaled Data:\n",
      " [[-1.22474487 -1.16247639]\n",
      " [ 0.          1.27872403]\n",
      " [ 1.22474487 -0.11624764]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data: [age, salary]\n",
    "data = np.array([[25, 50000], [45, 120000], [65, 80000]])\n",
    "\n",
    "# 1. Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 2. Fit the scaler to the data (it learns the mean and standard deviation)\n",
    "scaler.fit(data)\n",
    "\n",
    "# 3. Transform the data\n",
    "scaled_data = scaler.transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"\\nScaled Data:\\n\", scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01059e7a-63fe-42ca-8063-80a57d4265f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a2a4854-0f6a-44b0-8782-c54f8d08154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing: Encoding Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c92e24-3553-4595-a0cd-ba6d1e3e8e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37525be3-8582-497f-9bb8-4074de8c1348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "113bf473-52d6-494c-a102-31fea3d2f9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " [['Red']\n",
      " ['Green']\n",
      " ['Blue']\n",
      " ['Green']]\n",
      "\n",
      "Encoded Data (Columns: Blue, Green, Red):\n",
      " [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Sample data: [Color]\n",
    "data = np.array([['Red'], ['Green'], ['Blue'], ['Green']])\n",
    "\n",
    "# 1. Initialize the encoder\n",
    "encoder = OneHotEncoder(sparse_output=False) # sparse=False gives a readable array\n",
    "\n",
    "# 2. Fit and transform the data\n",
    "encoded_data = encoder.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"\\nEncoded Data (Columns: Blue, Green, Red):\\n\", encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b00992-41ed-40fd-8084-4384ea1e2625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "048806b6-58c1-4071-8d71-be729aaa2657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assembling a Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2cf979-335c-4e42-bb49-2a2cff8ce171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f97902b4-6948-462c-842e-4416fb68114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project: Building a Full Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8caba3f-de6d-46b0-af46-772ef7c017ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f37cabf2-65d6-45ab-91b1-31e0e8b27837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='median')),\n",
      "                                                                  ('scaler',\n",
      "                                                                   StandardScaler())]),\n",
      "                                                  ['Age', 'Fare']),\n",
      "                                                 ('cat',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='most_frequent')),\n",
      "                                                                  ('onehot',\n",
      "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
      "                                                  ['Pclass', 'Sex',\n",
      "                                                   'Embarked'])])),\n",
      "                ('classifier', LogisticRegression())])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer # To handle missing values\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Assume 'titanic.csv' is loaded into a pandas DataFrame `df`\n",
    "# For simplicity, we'll select a few columns and drop rows with missing target\n",
    "# X = df[['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']]\n",
    "# y = df['Survived']\n",
    "\n",
    "# --- Let's create a dummy DataFrame for this example ---\n",
    "data = {'Pclass': [3, 1, 3], 'Sex': ['male', 'female', 'female'], 'Age': [22.0, 38.0, 26.0], 'Fare': [7.25, 71.28, 7.92], 'Embarked': ['S', 'C', 'S']}\n",
    "X = pd.DataFrame(data)\n",
    "\n",
    "# 1. Define which columns are which type\n",
    "numeric_features = ['Age', 'Fare']\n",
    "categorical_features = ['Pclass', 'Sex', 'Embarked']\n",
    "\n",
    "# 2. Create preprocessing steps for each type\n",
    "# For numeric: impute missing values with the median, then scale\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# For categorical: impute missing with most frequent, then one-hot encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# 3. Combine preprocessing steps with a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# 4. Create the final pipeline with a model\n",
    "# The pipeline will first run the 'preprocessor', then train the 'classifier'\n",
    "model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                 ('classifier', LogisticRegression())])\n",
    "\n",
    "print(model_pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
